{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommender using Tensorflow Recommenders\n",
    "$by:\\space Jeremiah\\space Chinyelugo$\n",
    "\n",
    "This notebook shows how we can build a simple recommender system using tensorflow recommenders. \n",
    "\n",
    "**Disclamer:** This notebook is by no means an exhaustive introduction to building recommendation systems. To understand the basics, please refer to google's [resources](https://www.tensorflow.org/recommenders/examples/basic_retrieval) which goes into detail on how to build recommendation systems using the movielens dataset. Extremly helpful!\n",
    "\n",
    "\n",
    "#### What is a Recommender system?\n",
    "A Recommender systems is a models, algorithm, or technique used to suggest items, products, or content to users based on their preferences or attributes. These systems analyze large amounts of user data, such as past behavior, ratings, purchase history, or browsing patterns, to generate personalized recommendations.\n",
    "\n",
    "Recommender systems are important because the help companies increase sales and conversions, help users discover new items or content and enhance user experience.\n",
    "\n",
    "#### How do recommender systems work?\n",
    "Recommender systems in practical applications typically consist of two main phases:\n",
    "\n",
    "The first stage, known as retrieval, focuses on selecting an initial group of hundreds of potential candidates from the entire pool of available options. The primary goal of this stage is to efficiently filter out any candidates that are unlikely to be of interest to the user. Due to the potentially large number of candidates involved, the retrieval model must be designed to perform computations swiftly and effectively.\n",
    "\n",
    "Following the retrieval stage is the ranking stage, which refines the outputs of the retrieval model to identify the best possible subset of recommendations. Its objective is to narrow down the set of items that the user might find appealing to a concise list of highly probable candidates.\n",
    "\n",
    "\n",
    "## Contents\n",
    "1. Importing the packages\n",
    "2. Preparing our dataset\n",
    "3. Model building\n",
    "4. Training and evaluating the model\n",
    "5. Creating a function that will recommend Books for a user based on their `User ID`, `Age`, and `Specific Author`\n",
    "\n",
    "<br/>\n",
    "\n",
    "**As mentioned above, recommendation systems consists of two parts, and in this notebook, we look at both parts (Retrieval & Ranking)**\n",
    "\n",
    "#### Data\n",
    "The [data](https://github.com/caserec/Datasets-for-Recommender-Systems/tree/master/Processed%20Datasets/BookCrossing) used for this project was gotten from GitHub. The Book Crossing dataset were collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the Book-Crossing community with kind permission from Ron Hornbaker, CTO of Humankind Systems. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tempfile\n",
    "import os\n",
    "import pyinputplus as pyip\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "\n",
    "ratings = pd.read_csv('../../../../../Downloads/book_crossing/book_crossing/book_ratings.dat', delimiter='\\s+')\n",
    "items = pd.read_excel('../../../../../Downloads/book_crossing/book_crossing/items_info.xlsx')\n",
    "users = pd.read_csv('../../../../../Downloads/book_crossing/book_crossing/users_info.dat', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns in the users dataframe and dropping features not relevant to this project\n",
    "\n",
    "users = users.reset_index()\n",
    "users.rename(columns={'index':'User-ID', 'User-ID':'Location', 'Location':'Age', 'Age':'nan'}, inplace=True)\n",
    "users.drop(['nan', 'Location'], axis=1, inplace=True)\n",
    "\n",
    "items = items[[' Book_ID','ISBN', 'Book-Title', 'Book-Author']]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So are we droping some features?**\n",
    "\n",
    "we drop some features because we only want features that will be available during inference or when being used by users. Remember we are building a recommendation system that should be able to recommend books for **Users** based on their user id and favourite author, so we \n",
    "we only include useful features that will available when the model has been deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>Book_ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6264</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6264</td>\n",
       "      <td>0553280325</td>\n",
       "      <td>Something Wicked This Way Comes</td>\n",
       "      <td>Ray Bradbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496</td>\n",
       "      <td>6264</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6264</td>\n",
       "      <td>0553280325</td>\n",
       "      <td>Something Wicked This Way Comes</td>\n",
       "      <td>Ray Bradbury</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating   Book_ID        ISBN                       Book-Title  \\\n",
       "0     1  6264     7.0      6264  0553280325  Something Wicked This Way Comes   \n",
       "1   496  6264     8.0      6264  0553280325  Something Wicked This Way Comes   \n",
       "\n",
       "    Book-Author  \n",
       "0  Ray Bradbury  \n",
       "1  Ray Bradbury  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging our datasets into one encompassing dataset\n",
    "\n",
    "df1 = pd.merge(ratings, items, left_on='item', right_on=' Book_ID')\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>Book_ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6264</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6264</td>\n",
       "      <td>0553280325</td>\n",
       "      <td>Something Wicked This Way Comes</td>\n",
       "      <td>Ray Bradbury</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4350</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4350</td>\n",
       "      <td>0345441184</td>\n",
       "      <td>The Mists of Avalon</td>\n",
       "      <td>MARION ZIMMER BRADLEY</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating   Book_ID        ISBN                       Book-Title  \\\n",
       "0     1  6264     7.0      6264  0553280325  Something Wicked This Way Comes   \n",
       "1     1  4350     7.0      4350  0345441184              The Mists of Avalon   \n",
       "\n",
       "             Book-Author  User-ID  Age  \n",
       "0           Ray Bradbury        1   24  \n",
       "1  MARION ZIMMER BRADLEY        1   24  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df1, users, left_on='user', right_on='User-ID')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drooping duplicate and non relevant features\n",
    "\n",
    "df.drop(['User-ID', 'item', ' Book_ID', 'ISBN'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user has 1,295 values\n",
      "rating has 10 values\n",
      "Book-Title has 14,016 values\n",
      "Book-Author has 8,500 values\n",
      "Age has 72 values\n"
     ]
    }
   ],
   "source": [
    "# checking the percentage of unique values we have in each feature\n",
    "\n",
    "for col in df.columns:\n",
    "    print(f\"{col:} has {df[col].nunique():,} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user           0\n",
       "rating         0\n",
       "Book-Title     0\n",
       "Book-Author    0\n",
       "Age            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62651 entries, 0 to 62655\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   user         62651 non-null  int64  \n",
      " 1   rating       62651 non-null  float64\n",
      " 2   Book-Title   62651 non-null  object \n",
      " 3   Book-Author  62651 non-null  object \n",
      " 4   Age          62651 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a brief look through our `Age` feature we have some users over 100 years and some as old as 206. So we need to trim our age fearture, by droppping instances where user is over the age 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Something Wicked This Way Comes</td>\n",
       "      <td>Ray Bradbury</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>The Mists of Avalon</td>\n",
       "      <td>MARION ZIMMER BRADLEY</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Sacred Sins</td>\n",
       "      <td>Nora Roberts</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>What a Wonderful World: A Lifetime of Recordings</td>\n",
       "      <td>Bob Thiele</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>A Coral Kiss</td>\n",
       "      <td>Jayne Ann Krentz</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  rating                                        Book-Title  \\\n",
       "0     1     7.0                   Something Wicked This Way Comes   \n",
       "1     1     7.0                               The Mists of Avalon   \n",
       "2     1     5.0                                       Sacred Sins   \n",
       "3     1     9.0  What a Wonderful World: A Lifetime of Recordings   \n",
       "4     1     6.0                                      A Coral Kiss   \n",
       "\n",
       "             Book-Author  Age  \n",
       "0           Ray Bradbury   24  \n",
       "1  MARION ZIMMER BRADLEY   24  \n",
       "2           Nora Roberts   24  \n",
       "3             Bob Thiele   24  \n",
       "4       Jayne Ann Krentz   24  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trimming age range\n",
    "\n",
    "df = df[~df['Age'].between(100, 300)]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our dataset, we need to convert our pandas dataframe to a tensorflow dataset object, and that is what the following cells entails.\n",
    "\n",
    "To create an effective model, proper preprocessing of string (str) and integer (int) features is of utmost importance. In machine learning, there are two predominant approaches to preprocess data, each with its own associated drawbacks.\n",
    "\n",
    "The first approach involves preprocessing the data prior to feeding it into the model. This method is often favored when operating on low-performance devices, such as laptops, as including a preprocessing step within the model can potentially impede training time. However, a notable drawback arises during the deployment of the model. In such scenarios, a separate preprocessing step must be developed and integrated into the deployment pipeline. Furthermore, if the preprocessing step encounters unfamiliar data, it may encounter difficulties in handling it appropriately, thereby leading to suboptimal model performance.\n",
    "\n",
    "The second approach entails integrating the preprocessing step directly within the model itself. While this may marginally impact training time, it offers the advantage of simplified model deployment. By incorporating the preprocessing step as an integral part of the model, the need for separate preprocessing code during deployment is eliminated, streamlining the overall process.\n",
    "\n",
    "It is essential to acknowledge that both approaches involve trade-offs, and the selection between them hinges upon factors such as available computational resources, deployment requisites, and the inherent characteristics of the data under consideration.\n",
    "\n",
    "We will be including our preprocessing step into our model, and to do that we need the unique values of each our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting categorical & numerical features to string & integer respectively\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in ['rating','Age']:\n",
    "        df[col] = df[col].astype(str)\n",
    "    else:\n",
    "        df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user           object\n",
       "rating          int32\n",
       "Book-Title     object\n",
       "Book-Author    object\n",
       "Age             int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting our df to dictionary\n",
    "\n",
    "df_dict = {name: np.array(val) for name, val in df.items()}\n",
    "\n",
    "# converting our dataframne dictionary\n",
    "data = tf.data.Dataset.from_tensor_slices(df_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text vex for book and author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a dictionary of unique values in our features\n",
    "\n",
    "vocabularies = {}\n",
    "\n",
    "for feature in df_dict:\n",
    "    if feature != 'rating':\n",
    "        vocab = np.unique(df_dict[feature])\n",
    "        vocabularies[feature] = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting book-title to a tensorflow dataset\n",
    "book_titles = tf.data.Dataset.from_tensor_slices(vocabularies['Book-Title'])\n",
    "book_authors = df['Book-Author'].unique()\n",
    "user_age = df['Age'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling and splitting our dataset into train, validation and test\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "shuffled = data.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(46_797)\n",
    "validation = shuffled.skip(46_797).take(9_359)\n",
    "test = shuffled.skip(56_156).take(6_240)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Building\n",
    "\n",
    "Recommender systems often consits of retrieval and ranking models. We will build both models in this section.\n",
    "\n",
    "In these models, the required feature preprocessing steps will be included. This will reduce the chances of error that might be introduced when deployed in production, and also make deployment easier.\n",
    "\n",
    "We will create 3 python classes:\n",
    "- *UserModel:* This class will be responsible for preprocessing the user's attributes using Embeddings\n",
    "- *TitleModel:* This class will preprocess the book titles in our dataset using Embeddings\n",
    "- *FullModel:* This class will incorporate output of the UserModel and TitleModel (i.e, Embeddings) to perform a retrieval and ranking task. The retrieval and ranking tasks will be created using tensorflow recommenders.\n",
    "\n",
    "**More info on the Entire model**\n",
    "\n",
    "Our model which incorprates all the classes mentioned above will include:\n",
    "- User atrributes embeddings like age, user-id, and book-author\n",
    "- Title embeddings\n",
    "- Deep & Cross Network\n",
    "- Dense layers\n",
    "- Retrieval task layer to retrieve top k categories that allign with the user's attributes\n",
    "- Ranking task layer to rank categories \n",
    "- `call()` method to build the model\n",
    "- `compute_loss()` method\n",
    "\n",
    "The User attributes embeddings reduce categorical features with large number of unique items into a more managable form. To create the embeddings, the features have to be passed to a LookupLayer which assigns an index to each unique value in the vocabulary we created earlier, which is then passed to an embedding layer that creates an n-dimensional representation our feature. In this case, we will be using a dimension of 32.\n",
    "\n",
    "Two different embeddings (integer and categorical) will be created based on the data type of our feature. \n",
    "\n",
    "The Deep & Cross Network layer is great for ranking tasks, where we have a lot of features and need additional information by feature crossing. By crossing our features, the model can learn more or identify patters about our data by looking at their interactions.\n",
    "\n",
    "The Dense layer contains several densely connected layers with neurons that allow arbitrary nonlinear mapping betwwen inputs and outputs. \n",
    "\n",
    "The retrieval task layer efficiently weeds out books that a user will not be interested in, by reducing the number of potential candidates.\n",
    "\n",
    "The ranking task layer ranks the candidates that were retrieved by the retrieval layer.\n",
    "\n",
    "The call method excutes various steps and creates the model\n",
    "\n",
    "The compute_loss method measures how well the model is performing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        max_tokens = 10_000\n",
    "        \n",
    "        # 1. User ID\n",
    "        self.user_id_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=vocabularies['user'],\n",
    "                mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(vocabularies['user'])+1, 32)\n",
    "        ])\n",
    "             \n",
    "        \n",
    "        #2. Book Authors\n",
    "        self.author_vectorizer = keras.layers.TextVectorization(max_tokens=max_tokens)\n",
    "        self.author_vectorizer.adapt(book_authors)\n",
    "        self.author_text_embedding = keras.Sequential([\n",
    "            self.author_vectorizer,\n",
    "            keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "            keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        \n",
    "        self.author_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=vocabularies['Book-Author'],\n",
    "                mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(vocabularies['Book-Author'])+1, 32)\n",
    "        ])\n",
    "         \n",
    "        \n",
    "        # 3. User age\n",
    "        self.normalized_age = keras.layers.Normalization()\n",
    "        self.normalized_age.adapt(vocabularies['Age'].reshape(-1,1))\n",
    "        \n",
    "    # call method passes out input features to the embeddings above, excutes them and returns the output\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        return tf.concat([\n",
    "            self.user_id_embedding(inputs['user']),\n",
    "            self.author_embedding(inputs['Book-Author']),\n",
    "            self.author_text_embedding(inputs['Book-Author']),\n",
    "            tf.reshape(self.normalized_age(inputs['Age']), (-1,1))\n",
    "        ], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "        max_tokens = 10_000\n",
    "        \n",
    "        #1. Book-Titles\n",
    "        self.book_vectorizer = keras.layers.TextVectorization(max_tokens=max_tokens)\n",
    "        self.book_vectorizer.adapt(book_titles)\n",
    "        self.book_text_embedding = keras.Sequential([\n",
    "            self.book_vectorizer,\n",
    "            keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "            keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        \n",
    "        self.book_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=vocabularies['Book-Title'],\n",
    "                mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(vocabularies['Book-Title'])+1, 32)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    # call method passes category to the embedding layer above, executes it and returns the output embeddings\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        return tf.concat([\n",
    "            self.book_embedding(inputs),\n",
    "            self.book_text_embedding(inputs),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "\n",
    "class FullModel(tfrs.models.Model):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "        # handles how much weight we want to assign to the rating and retrieval task when computing loss\n",
    "        self.rating_weight = 0.5\n",
    "        self.retrieval_weight = 0.5\n",
    "        \n",
    "        #User model\n",
    "        self.user_model = tf.keras.Sequential([\n",
    "            UserModel(),\n",
    "            tf.keras.layers.Dense(32),\n",
    "        ])\n",
    "        \n",
    "        # Category model\n",
    "        self.title_model = tf.keras.Sequential([\n",
    "            TitleModel(),\n",
    "            tf.keras.layers.Dense(32)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        # Deep & Cross layer\n",
    "        self._cross_layer = tfrs.layers.dcn.Cross(projection_dim=None, kernel_initializer='he_normal')\n",
    "        \n",
    "        # Dense layers with l2 regularization to prevent overfitting\n",
    "        self._deep_layers = [\n",
    "            keras.layers.Dense(512, activation='relu', kernel_regularizer='l2'),\n",
    "            keras.layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(128, activation='relu', kernel_regularizer='l2'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n",
    "            keras.layers.Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "        ]\n",
    "        \n",
    "        # output layer\n",
    "        self._logit_layer = keras.layers.Dense(1)\n",
    "    \n",
    "        # Multi-task Retrieval & Ranking\n",
    "        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=book_titles.batch(128).map(self.title_model)\n",
    "            )\n",
    "        )\n",
    "       \n",
    "            \n",
    "    def call(self, features) -> tf.Tensor:\n",
    "        user_embeddings = self.user_model({\n",
    "            'user': features['user'],\n",
    "            'Book-Author': features['Book-Author'],\n",
    "            'Age': features['Age'],\n",
    "        })\n",
    "        \n",
    "        \n",
    "        title_embeddings = self.title_model(\n",
    "            features['Book-Title']\n",
    "        )\n",
    "        \n",
    "        x = self._cross_layer(tf.concat([\n",
    "                user_embeddings,\n",
    "                title_embeddings], axis=1))\n",
    "        \n",
    "        for layer in self._deep_layers.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        \n",
    "        return (\n",
    "            user_embeddings, \n",
    "            title_embeddings,\n",
    "            self._logit_layer(x)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def compute_loss(self, features, training=False) -> tf.Tensor:\n",
    "        user_embeddings, title_embeddings, rating_predictions = self.call(features)\n",
    "        # Retrieval loss\n",
    "        retrieval_loss = self.retrieval_task(user_embeddings, title_embeddings)\n",
    "        # Rating loss\n",
    "        rating_loss = self.rating_task(\n",
    "            labels=features['rating'],\n",
    "            predictions=rating_predictions\n",
    "        )\n",
    "        \n",
    "        # Combine two losses with hyper-parameters (to be tuned)\n",
    "        return (self.rating_weight * rating_loss + self.retrieval_weight * retrieval_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching and caching our datasets to improve performance\n",
    "\n",
    "cached_train = train.shuffle(143_000).batch(2000).cache()\n",
    "cached_validation = validation.shuffle(30_000).batch(2000).cache()\n",
    "cached_test = test.batch(1000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 20s 674ms/step - root_mean_squared_error: 26.0196 - factorized_top_k/top_1_categorical_accuracy: 0.0145 - factorized_top_k/top_5_categorical_accuracy: 0.0451 - factorized_top_k/top_10_categorical_accuracy: 0.0640 - factorized_top_k/top_50_categorical_accuracy: 0.1299 - factorized_top_k/top_100_categorical_accuracy: 0.1755 - loss: 6909.3458 - regularization_loss: 16.1247 - total_loss: 6925.4704 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.0479 - val_factorized_top_k/top_5_categorical_accuracy: 0.1664 - val_factorized_top_k/top_10_categorical_accuracy: 0.2250 - val_factorized_top_k/top_50_categorical_accuracy: 0.3597 - val_factorized_top_k/top_100_categorical_accuracy: 0.4222 - val_loss: 3565.2307 - val_regularization_loss: 16.9996 - val_total_loss: 3582.2302\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 16s 675ms/step - root_mean_squared_error: 2.2396 - factorized_top_k/top_1_categorical_accuracy: 0.0931 - factorized_top_k/top_5_categorical_accuracy: 0.2681 - factorized_top_k/top_10_categorical_accuracy: 0.3483 - factorized_top_k/top_50_categorical_accuracy: 0.5176 - factorized_top_k/top_100_categorical_accuracy: 0.5843 - loss: 4202.5793 - regularization_loss: 16.4394 - total_loss: 4219.0187 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.1382 - val_factorized_top_k/top_5_categorical_accuracy: 0.3950 - val_factorized_top_k/top_10_categorical_accuracy: 0.4798 - val_factorized_top_k/top_50_categorical_accuracy: 0.6160 - val_factorized_top_k/top_100_categorical_accuracy: 0.6686 - val_loss: 2391.9353 - val_regularization_loss: 15.8872 - val_total_loss: 2407.8225\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 16s 653ms/step - root_mean_squared_error: 1.8882 - factorized_top_k/top_1_categorical_accuracy: 0.1812 - factorized_top_k/top_5_categorical_accuracy: 0.5005 - factorized_top_k/top_10_categorical_accuracy: 0.6027 - factorized_top_k/top_50_categorical_accuracy: 0.7606 - factorized_top_k/top_100_categorical_accuracy: 0.8071 - loss: 2514.4395 - regularization_loss: 15.3900 - total_loss: 2529.8294 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.1949 - val_factorized_top_k/top_5_categorical_accuracy: 0.5362 - val_factorized_top_k/top_10_categorical_accuracy: 0.6275 - val_factorized_top_k/top_50_categorical_accuracy: 0.7576 - val_factorized_top_k/top_100_categorical_accuracy: 0.7944 - val_loss: 1778.9348 - val_regularization_loss: 14.9214 - val_total_loss: 1793.8562\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 15s 611ms/step - root_mean_squared_error: 1.8045 - factorized_top_k/top_1_categorical_accuracy: 0.2300 - factorized_top_k/top_5_categorical_accuracy: 0.6382 - factorized_top_k/top_10_categorical_accuracy: 0.7430 - factorized_top_k/top_50_categorical_accuracy: 0.8787 - factorized_top_k/top_100_categorical_accuracy: 0.9112 - loss: 1573.0466 - regularization_loss: 14.5014 - total_loss: 1587.5480 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.2323 - val_factorized_top_k/top_5_categorical_accuracy: 0.6111 - val_factorized_top_k/top_10_categorical_accuracy: 0.7072 - val_factorized_top_k/top_50_categorical_accuracy: 0.8350 - val_factorized_top_k/top_100_categorical_accuracy: 0.8633 - val_loss: 1498.5876 - val_regularization_loss: 14.1036 - val_total_loss: 1512.6913\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 15s 628ms/step - root_mean_squared_error: 1.7684 - factorized_top_k/top_1_categorical_accuracy: 0.2410 - factorized_top_k/top_5_categorical_accuracy: 0.7183 - factorized_top_k/top_10_categorical_accuracy: 0.8240 - factorized_top_k/top_50_categorical_accuracy: 0.9461 - factorized_top_k/top_100_categorical_accuracy: 0.9681 - loss: 1052.4055 - regularization_loss: 13.7467 - total_loss: 1066.1522 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.2756 - val_factorized_top_k/top_5_categorical_accuracy: 0.6964 - val_factorized_top_k/top_10_categorical_accuracy: 0.7887 - val_factorized_top_k/top_50_categorical_accuracy: 0.8818 - val_factorized_top_k/top_100_categorical_accuracy: 0.8959 - val_loss: 1405.9733 - val_regularization_loss: 13.4062 - val_total_loss: 1419.3794\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 15s 647ms/step - root_mean_squared_error: 1.7540 - factorized_top_k/top_1_categorical_accuracy: 0.2641 - factorized_top_k/top_5_categorical_accuracy: 0.7985 - factorized_top_k/top_10_categorical_accuracy: 0.8992 - factorized_top_k/top_50_categorical_accuracy: 0.9835 - factorized_top_k/top_100_categorical_accuracy: 0.9927 - loss: 790.9816 - regularization_loss: 13.0999 - total_loss: 804.0816 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.2901 - val_factorized_top_k/top_5_categorical_accuracy: 0.7192 - val_factorized_top_k/top_10_categorical_accuracy: 0.8078 - val_factorized_top_k/top_50_categorical_accuracy: 0.8921 - val_factorized_top_k/top_100_categorical_accuracy: 0.9019 - val_loss: 1391.1697 - val_regularization_loss: 12.8048 - val_total_loss: 1403.9745\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 16s 679ms/step - root_mean_squared_error: 1.7399 - factorized_top_k/top_1_categorical_accuracy: 0.2906 - factorized_top_k/top_5_categorical_accuracy: 0.8390 - factorized_top_k/top_10_categorical_accuracy: 0.9292 - factorized_top_k/top_50_categorical_accuracy: 0.9928 - factorized_top_k/top_100_categorical_accuracy: 0.9976 - loss: 672.7765 - regularization_loss: 12.5409 - total_loss: 685.3174 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3108 - val_factorized_top_k/top_5_categorical_accuracy: 0.7350 - val_factorized_top_k/top_10_categorical_accuracy: 0.8178 - val_factorized_top_k/top_50_categorical_accuracy: 0.8985 - val_factorized_top_k/top_100_categorical_accuracy: 0.9061 - val_loss: 1379.3684 - val_regularization_loss: 12.2827 - val_total_loss: 1391.6511\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 16s 663ms/step - root_mean_squared_error: 1.7288 - factorized_top_k/top_1_categorical_accuracy: 0.2984 - factorized_top_k/top_5_categorical_accuracy: 0.8672 - factorized_top_k/top_10_categorical_accuracy: 0.9488 - factorized_top_k/top_50_categorical_accuracy: 0.9973 - factorized_top_k/top_100_categorical_accuracy: 0.9992 - loss: 586.9575 - regularization_loss: 12.0511 - total_loss: 599.0086 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3186 - val_factorized_top_k/top_5_categorical_accuracy: 0.7531 - val_factorized_top_k/top_10_categorical_accuracy: 0.8368 - val_factorized_top_k/top_50_categorical_accuracy: 0.9031 - val_factorized_top_k/top_100_categorical_accuracy: 0.9069 - val_loss: 1385.1936 - val_regularization_loss: 11.8228 - val_total_loss: 1397.0165\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 15s 639ms/step - root_mean_squared_error: 1.7318 - factorized_top_k/top_1_categorical_accuracy: 0.2987 - factorized_top_k/top_5_categorical_accuracy: 0.8896 - factorized_top_k/top_10_categorical_accuracy: 0.9605 - factorized_top_k/top_50_categorical_accuracy: 0.9988 - factorized_top_k/top_100_categorical_accuracy: 0.9998 - loss: 521.6639 - regularization_loss: 11.6201 - total_loss: 533.2840 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3296 - val_factorized_top_k/top_5_categorical_accuracy: 0.7526 - val_factorized_top_k/top_10_categorical_accuracy: 0.8361 - val_factorized_top_k/top_50_categorical_accuracy: 0.9022 - val_factorized_top_k/top_100_categorical_accuracy: 0.9075 - val_loss: 1421.5547 - val_regularization_loss: 11.4173 - val_total_loss: 1432.9719\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 17s 726ms/step - root_mean_squared_error: 1.7267 - factorized_top_k/top_1_categorical_accuracy: 0.2857 - factorized_top_k/top_5_categorical_accuracy: 0.9053 - factorized_top_k/top_10_categorical_accuracy: 0.9701 - factorized_top_k/top_50_categorical_accuracy: 0.9996 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 477.2394 - regularization_loss: 11.2340 - total_loss: 488.4734 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3226 - val_factorized_top_k/top_5_categorical_accuracy: 0.7564 - val_factorized_top_k/top_10_categorical_accuracy: 0.8350 - val_factorized_top_k/top_50_categorical_accuracy: 0.9004 - val_factorized_top_k/top_100_categorical_accuracy: 0.9070 - val_loss: 1479.1715 - val_regularization_loss: 11.0518 - val_total_loss: 1490.2234\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 18s 747ms/step - root_mean_squared_error: 1.7251 - factorized_top_k/top_1_categorical_accuracy: 0.2828 - factorized_top_k/top_5_categorical_accuracy: 0.9153 - factorized_top_k/top_10_categorical_accuracy: 0.9745 - factorized_top_k/top_50_categorical_accuracy: 0.9997 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 437.6558 - regularization_loss: 10.8860 - total_loss: 448.5417 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3291 - val_factorized_top_k/top_5_categorical_accuracy: 0.7507 - val_factorized_top_k/top_10_categorical_accuracy: 0.8344 - val_factorized_top_k/top_50_categorical_accuracy: 0.8995 - val_factorized_top_k/top_100_categorical_accuracy: 0.9063 - val_loss: 1530.7910 - val_regularization_loss: 10.7207 - val_total_loss: 1541.5117\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 15s 625ms/step - root_mean_squared_error: 1.7247 - factorized_top_k/top_1_categorical_accuracy: 0.2646 - factorized_top_k/top_5_categorical_accuracy: 0.9230 - factorized_top_k/top_10_categorical_accuracy: 0.9794 - factorized_top_k/top_50_categorical_accuracy: 0.9997 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 402.3351 - regularization_loss: 10.5701 - total_loss: 412.9052 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3172 - val_factorized_top_k/top_5_categorical_accuracy: 0.7495 - val_factorized_top_k/top_10_categorical_accuracy: 0.8291 - val_factorized_top_k/top_50_categorical_accuracy: 0.8989 - val_factorized_top_k/top_100_categorical_accuracy: 0.9057 - val_loss: 1594.5662 - val_regularization_loss: 10.4194 - val_total_loss: 1604.9856\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 15s 626ms/step - root_mean_squared_error: 1.7225 - factorized_top_k/top_1_categorical_accuracy: 0.2531 - factorized_top_k/top_5_categorical_accuracy: 0.9295 - factorized_top_k/top_10_categorical_accuracy: 0.9817 - factorized_top_k/top_50_categorical_accuracy: 0.9999 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 374.0722 - regularization_loss: 10.2814 - total_loss: 384.3535 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3089 - val_factorized_top_k/top_5_categorical_accuracy: 0.7467 - val_factorized_top_k/top_10_categorical_accuracy: 0.8244 - val_factorized_top_k/top_50_categorical_accuracy: 0.8973 - val_factorized_top_k/top_100_categorical_accuracy: 0.9045 - val_loss: 1649.8170 - val_regularization_loss: 10.1430 - val_total_loss: 1659.9600\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 15s 609ms/step - root_mean_squared_error: 1.7201 - factorized_top_k/top_1_categorical_accuracy: 0.2466 - factorized_top_k/top_5_categorical_accuracy: 0.9358 - factorized_top_k/top_10_categorical_accuracy: 0.9845 - factorized_top_k/top_50_categorical_accuracy: 0.9999 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 350.6345 - regularization_loss: 10.0155 - total_loss: 360.6500 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3116 - val_factorized_top_k/top_5_categorical_accuracy: 0.7438 - val_factorized_top_k/top_10_categorical_accuracy: 0.8246 - val_factorized_top_k/top_50_categorical_accuracy: 0.8948 - val_factorized_top_k/top_100_categorical_accuracy: 0.9033 - val_loss: 1701.1411 - val_regularization_loss: 9.8874 - val_total_loss: 1711.0284\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 15s 651ms/step - root_mean_squared_error: 1.7216 - factorized_top_k/top_1_categorical_accuracy: 0.2387 - factorized_top_k/top_5_categorical_accuracy: 0.9403 - factorized_top_k/top_10_categorical_accuracy: 0.9864 - factorized_top_k/top_50_categorical_accuracy: 0.9999 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 330.3613 - regularization_loss: 9.7697 - total_loss: 340.1310 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3169 - val_factorized_top_k/top_5_categorical_accuracy: 0.7416 - val_factorized_top_k/top_10_categorical_accuracy: 0.8197 - val_factorized_top_k/top_50_categorical_accuracy: 0.8932 - val_factorized_top_k/top_100_categorical_accuracy: 0.9028 - val_loss: 1764.3545 - val_regularization_loss: 9.6511 - val_total_loss: 1774.0056\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 15s 615ms/step - root_mean_squared_error: 1.7153 - factorized_top_k/top_1_categorical_accuracy: 0.2282 - factorized_top_k/top_5_categorical_accuracy: 0.9435 - factorized_top_k/top_10_categorical_accuracy: 0.9878 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 314.1864 - regularization_loss: 9.5406 - total_loss: 323.7270 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3062 - val_factorized_top_k/top_5_categorical_accuracy: 0.7360 - val_factorized_top_k/top_10_categorical_accuracy: 0.8171 - val_factorized_top_k/top_50_categorical_accuracy: 0.8914 - val_factorized_top_k/top_100_categorical_accuracy: 0.9020 - val_loss: 1815.1846 - val_regularization_loss: 9.4291 - val_total_loss: 1824.6138\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 14s 599ms/step - root_mean_squared_error: 1.7166 - factorized_top_k/top_1_categorical_accuracy: 0.2284 - factorized_top_k/top_5_categorical_accuracy: 0.9472 - factorized_top_k/top_10_categorical_accuracy: 0.9887 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 301.4349 - regularization_loss: 9.3264 - total_loss: 310.7613 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.2995 - val_factorized_top_k/top_5_categorical_accuracy: 0.7375 - val_factorized_top_k/top_10_categorical_accuracy: 0.8164 - val_factorized_top_k/top_50_categorical_accuracy: 0.8910 - val_factorized_top_k/top_100_categorical_accuracy: 0.9007 - val_loss: 1875.5316 - val_regularization_loss: 9.2220 - val_total_loss: 1884.7537\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 14s 595ms/step - root_mean_squared_error: 1.7168 - factorized_top_k/top_1_categorical_accuracy: 0.2200 - factorized_top_k/top_5_categorical_accuracy: 0.9494 - factorized_top_k/top_10_categorical_accuracy: 0.9898 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 290.6789 - regularization_loss: 9.1249 - total_loss: 299.8038 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.2986 - val_factorized_top_k/top_5_categorical_accuracy: 0.7321 - val_factorized_top_k/top_10_categorical_accuracy: 0.8138 - val_factorized_top_k/top_50_categorical_accuracy: 0.8901 - val_factorized_top_k/top_100_categorical_accuracy: 0.8999 - val_loss: 1922.7092 - val_regularization_loss: 9.0263 - val_total_loss: 1931.7355\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 14s 580ms/step - root_mean_squared_error: 1.7288 - factorized_top_k/top_1_categorical_accuracy: 0.2184 - factorized_top_k/top_5_categorical_accuracy: 0.9514 - factorized_top_k/top_10_categorical_accuracy: 0.9910 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 282.7436 - regularization_loss: 8.9364 - total_loss: 291.6800 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.3013 - val_factorized_top_k/top_5_categorical_accuracy: 0.7314 - val_factorized_top_k/top_10_categorical_accuracy: 0.8113 - val_factorized_top_k/top_50_categorical_accuracy: 0.8890 - val_factorized_top_k/top_100_categorical_accuracy: 0.8997 - val_loss: 1974.1592 - val_regularization_loss: 8.8460 - val_total_loss: 1983.0051\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 14s 586ms/step - root_mean_squared_error: 1.7106 - factorized_top_k/top_1_categorical_accuracy: 0.2143 - factorized_top_k/top_5_categorical_accuracy: 0.9525 - factorized_top_k/top_10_categorical_accuracy: 0.9915 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 275.1565 - regularization_loss: 8.7581 - total_loss: 283.9146 - val_root_mean_squared_error: nan - val_factorized_top_k/top_1_categorical_accuracy: 0.2981 - val_factorized_top_k/top_5_categorical_accuracy: 0.7264 - val_factorized_top_k/top_10_categorical_accuracy: 0.8108 - val_factorized_top_k/top_50_categorical_accuracy: 0.8880 - val_factorized_top_k/top_100_categorical_accuracy: 0.8989 - val_loss: 2019.4722 - val_regularization_loss: 8.6697 - val_total_loss: 2028.1420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ce6be73340>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling our FullModel and training it\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# calling and training our model\n",
    "\n",
    "model = FullModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "model.fit(cached_train, validation_data=cached_validation, epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating our model on our test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(cached_test, return_dict=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root_mean_squared_error': 1.718819260597229,\n",
       " 'factorized_top_k/top_1_categorical_accuracy': 0.29471153020858765,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.7192307710647583,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.7990384697914124,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.8826923370361328,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.8939102292060852,\n",
       " 'loss': 273.7434997558594,\n",
       " 'regularization_loss': 8.669746398925781,\n",
       " 'total_loss': 282.4132385253906}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 1.719\n",
      "Top 1 accuracy: 29.47%\n",
      "Top 5 accuracy: 71.92%\n",
      "Top 10 accuracy: 79.90%\n",
      "Top 50 accuracy: 88.27%\n",
      "Top 100 accuracy: 89.39%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Root mean square error: {scores['root_mean_squared_error']:.3f}\")\n",
    "print(f\"Top 1 accuracy: {scores['factorized_top_k/top_1_categorical_accuracy']:.2%}\")\n",
    "print(f\"Top 5 accuracy: {scores['factorized_top_k/top_5_categorical_accuracy']:.2%}\")\n",
    "print(f\"Top 10 accuracy: {scores['factorized_top_k/top_10_categorical_accuracy']:.2%}\")\n",
    "print(f\"Top 50 accuracy: {scores['factorized_top_k/top_50_categorical_accuracy']:.2%}\")\n",
    "print(f\"Top 100 accuracy: {scores['factorized_top_k/top_100_categorical_accuracy']:.2%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what do these metrics mean?**\n",
    "\n",
    "In the context of recommender systems, \n",
    "- Root Mean Square Error (RMSE): Measures the average prediction error between the recommended ratings and actual ratings. Lower values indicate better accuracy.\n",
    "\n",
    "- Top 1 Accuracy: Represents the percentage of times the top-ranked recommendation matches the user's preference.\n",
    "\n",
    "- Top 5 Accuracy: Represents the percentage of instances where the user's preferred item is within the top 5 recommendations.\n",
    "\n",
    "- Top 10 Accuracy: Represents the percentage of times the user's preferred item appears in the top 10 recommendations.\n",
    "\n",
    "- Top 50 Accuracy: Represents the percentage of cases where the user's preferred item is among the top 50 recommendations.\n",
    "\n",
    " - Top 100 Accuracy: Represents the percentage of times the user's preferred item appears in the top 100 recommendations.\n",
    "\n",
    " Overall, our model performs well, with noticeable accuracy percentages and root mean square error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating a function that will recommend Books for a user based on their `User ID`, `Age`, and `Specific Author`\n",
    "\n",
    "**Note:** The function below will take the attributes listed above, validate them using a custom function to ensure passed inputs either exist within a range of integer values or authors and user-id exist in our vocabularies.\n",
    "\n",
    "Sure we could still recommend books for users who are not in our vocabularies (all we have to do is get rid of the validation functions), but for this project we will validate inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our input validation functions\n",
    "\n",
    "def validate_number(value):\n",
    "    try:\n",
    "        number = int(value)\n",
    "        if number in range(0,100):\n",
    "            return number\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Age\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Invalid Age\")\n",
    "\n",
    "\n",
    "def validate_author(value):\n",
    "    if value in vocabularies['Book-Author']:\n",
    "        return value\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Author Name\")\n",
    "    \n",
    "    \n",
    "def validate_user(value):\n",
    "    if value in vocabularies['user']:\n",
    "        return value\n",
    "    else:\n",
    "        raise ValueError(\"Invalid User-ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our recommendation functions\n",
    "\n",
    "def Recommend():\n",
    "    input_user = pyip.inputCustom(validate_user, prompt=\"Enter your User-ID: \\n\")\n",
    "    input_author = pyip.inputCustom(validate_author, prompt=\"Enter an Author name: \\n\")\n",
    "    input_age = pyip.inputCustom(validate_number, prompt=\"Enter your Age: \\n\")\n",
    "    top_k = pyip.inputNum(\"Number of recommendations: \\n\")\n",
    "    \n",
    "    print(f\"\\nGetting your {top_k} book recommendations. Please be patient\")\n",
    "    print(\"=================================================================================================================================\")\n",
    "    \n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model, k=top_k)\n",
    "    index.index_from_dataset(\n",
    "    tf.data.Dataset.zip((book_titles.batch(1000), book_titles.batch(1000).map(model.title_model)))\n",
    "    )\n",
    "    \n",
    "    raw_input = {\n",
    "        'Age': input_age,\n",
    "        'Book-Author': input_author,\n",
    "        'user': input_user\n",
    "    }\n",
    "    \n",
    "    input_dict = {key: tf.constant(np.array([value])) for key, value in raw_input.items()}\n",
    "    \n",
    "    _, titles = index(input_dict)\n",
    "    \n",
    "    test_rating = {}\n",
    "    for book in titles.numpy()[0]:\n",
    "        raw_input['Book-Title'] = book\n",
    "\n",
    "        input_dict = {key: tf.constant(np.array([value])) for key, value in raw_input.items()}\n",
    "\n",
    "        trained_movie_embeddings, trained_user_embeddings, predicted_rating = model(input_dict)\n",
    "        test_rating[book] = predicted_rating\n",
    "\n",
    "\n",
    "    sorted_dict = sorted(test_rating.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    \n",
    "    print(\"=================================================================================================================================\")\n",
    "    print(f\"Top {top_k} recommendations for User: {input_user}\\n\")\n",
    "    for i, (k, v) in enumerate(sorted_dict):\n",
    "        print(' '*2,'-',k,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your User-ID: \n",
      "Enter an Author name: \n",
      "Enter your Age: \n",
      "Number of recommendations: \n",
      "\n",
      "Getting your 10 book recommendations. Please be patient\n",
      "=================================================================================================================================\n",
      "=================================================================================================================================\n",
      "Top 10 recommendations for User: 2376\n",
      "\n",
      "   - b'Fahrenheit 451 / Fahrenheit 451'\n",
      "   - b'Parsifal Mosaic'\n",
      "   - b'Martian Chronicles'\n",
      "   - b\"The Vintage Bradbury: Ray Bradbury's Own Selection of His Best Stories\"\n",
      "   - b'Something Wicked This Way Comes'\n",
      "   - b'Something Wicked / E.X. Ferrars.'\n",
      "   - b'The Halloween Tree'\n",
      "   - b'Fahrenheit 451 - T.D. -'\n",
      "   - b'Dandelion Wine'\n",
      "   - b\"A journey to the center of the earth (World's best reading)\"\n"
     ]
    }
   ],
   "source": [
    "# calling our recommendation function\n",
    "\n",
    "Recommend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
